<!DOCTYPE html><html><head><meta charSet="utf-8"/><title>Rafael Eyng&#x27;s Blog</title><link href="//fonts.googleapis.com/css?family=Merriweather:900,900italic,300,300italic" rel="stylesheet" type="text/css"/><link rel="canonical" href="https://rafaeleyng.github.io/"/><meta name="description" content=""/><meta name="language" content="en"/><meta name="content-language" content="en"/><meta name="author" content="Rafael Eyng"/><meta name="keywords" content="software, development, javascript, github, node, docker, blog"/><meta name="viewport" content="width=device-width, initial-scale=1"/><meta name="next-head-count" content="10"/><link rel="preload" href="/_next/static/38LWUFnuUIJvM1_FqAo7H/pages/index.js" as="script"/><link rel="preload" href="/_next/static/38LWUFnuUIJvM1_FqAo7H/pages/_app.js" as="script"/><link rel="preload" href="/_next/static/runtime/webpack-9369c5c69dbf6d4912cb.js" as="script"/><link rel="preload" href="/_next/static/chunks/commons.e65592b2d799ba7332ae.js" as="script"/><link rel="preload" href="/_next/static/runtime/main-b1511fc684da8c38d25b.js" as="script"/><link rel="preload" href="/_next/static/css/commons.1c2d60be.chunk.css" as="style"/><link rel="stylesheet" href="/_next/static/css/commons.1c2d60be.chunk.css"/></head><body><div id="__next"><div class="container"><header class="site-header single-column"><a class="blog-title" href="/">Rafael Eyng&#x27;s Blog</a><nav class="blog-menu"><a href="https://rafaeleyng.github.io/me" target="_blank">About</a><a href="https://github.com/rafaeleyng/rafaeleyng.github.io" target="_blank">Github</a></nav></header></div><main class="container single-column"><ul class="post-list"><li><p class="post-meta"><time dateTime="2019-11-24">Nov 24, 2019</time></p><a class="post-link" href="/redis-pipelining-transactions-and-lua-scripts"><h2 class="post-title">Redis: Pipelining, Transactions and Lua Scripts</h2><p class="post-summary">Redis offers 3 ways of grouping commands: pipelining, transactions and Lua scripts. The subtleties of using one instead of the other are explored on this post.
</p></a></li><li><p class="post-meta"><time dateTime="2019-11-24">Nov 24, 2019</time></p><a class="post-link" href="/redis-replication-and-partitioning"><h2 class="post-title">Redis: Replication and Partitioning</h2><p class="post-summary">Replication and partitioning are techniques that build the foundation of using Redis as a distributed system, and prepare the way for more complex abstractions like Redis Sentinel and Redis Cluster
</p></a></li><li><p class="post-meta"><time dateTime="2017-01-07">Jan 7, 2017</time></p><a class="post-link" href="/using-nexus-3-as-your-repository-part-3-docker-images"><h2 class="post-title">Using Nexus 3 as Your Repository - Part 3: Docker Images</h2><p class="post-summary">In this third post of the series, we will setup Nexus 3 to use it as Docker private registry and as a proxy to Docker Hub.
</p></a></li><li><p class="post-meta"><time dateTime="2016-12-03">Dec 3, 2016</time></p><a class="post-link" href="/using-nexus-3-as-your-repository-part-2-npm-packages"><h2 class="post-title">Using Nexus 3 as Your Repository - Part 2: Npm Packages</h2><p class="post-summary">In this second post of the series, I&#x27;ll show you how to setup Nexus 3 and configure it to use it as a private npm registry and as a proxy to the official registry.
</p></a></li><li><p class="post-meta"><time dateTime="2016-11-23">Nov 23, 2016</time></p><a class="post-link" href="/using-nexus-3-as-your-repository-part-1-maven-artifacts"><h2 class="post-title">Using Nexus 3 as Your Repository - Part 1: Maven Artifacts</h2><p class="post-summary">Learn how to setup Nexus 3 and configure it to use it both as a Maven private repository and as a proxy to Maven Central and other repos
</p></a></li><li><p class="post-meta"><time dateTime="2016-11-19">Nov 19, 2016</time></p><a class="post-link" href="/monitoring-your-application-status-with-cabot"><h2 class="post-title">Monitoring Your Application Status With Cabot</h2><p class="post-summary">How to setup a status check for your applications
</p></a></li><li><p class="post-meta"><time dateTime="2016-01-14">Jan 14, 2016</time></p><a class="post-link" href="/javascript-code-coverage-with-instanbul-and-coveralls"><h2 class="post-title">JavaScript Code Coverage With Instanbul and Coveralls</h2><p class="post-summary">A fast lane to set up code coverage in your project
</p></a></li><li><p class="post-meta"><time dateTime="2016-01-09">Jan 9, 2016</time></p><a class="post-link" href="/simple-git-hooks-with-ghooks"><h2 class="post-title">Simple Git Hooks With Ghooks</h2><p class="post-summary">Learn how to use git hooks easily to leverage best practices in your workflow.
</p></a></li><li><p class="post-meta"><time dateTime="2015-10-30">Oct 30, 2015</time></p><a class="post-link" href="/optimizing-assets-in-jekyll-generated-gh-pages"><h2 class="post-title">Optimizing Assets in Jekyll-generated gh-pages</h2><p class="post-summary">Learn how to optimize assets for your Jekyll site without having to rely on plugins.
</p></a></li><li><p class="post-meta"><time dateTime="2015-07-21">Jul 21, 2015</time></p><a class="post-link" href="/practical-guide-to-functional-programming"><h2 class="post-title">Practical Guide to Functional Programming</h2><p class="post-summary">Functional Programming lies somewhere between Computer Science and Mathematics. It is a pretty dense and theoric subject, so the goal here is to show you how to start using some classic functions for collection manipulation in your everyday code.
</p></a></li></ul></main></div><script id="__NEXT_DATA__" type="application/json">{"dataManager":"[]","props":{"pageProps":{"posts":[{"document":{"content":"\nRedis offers 3 ways of grouping commands: pipelining, transactions and Lua scripts. The subtleties of using one instead of the other are explored on this post, in terms of the benefits they present, their limitations and atomicity.\n\n## Pipelining\n\nThink of pipelining as purely an optimization for sending multiple commands at a lower computational cost. Is the simplest of the three and offers less guarantees.\n\nPipelining in Redis consists of sending multiple commands to the server in the same message, separating commands by newline. You can test this (assuming you have Redis running locally on default port 6379) with `printf \"INCR x\\r\\nINCR x\\r\\n\" | nc localhost 6379`. This sends a single message containing two commands, separated by newlines. It is sometimes referred as \"batching\" in some libraries. Note that there is no special commands to mark the start or the end of a pipeline. It is really just a bunch of commands grouped together.\n\nThe server buffers all the answers in memory and sends all at once when the pipeline is done. So the pipeline size must not be too big that the server will have to hold a lot of answers in memory. Using a few thousands of commands inside a pipeline is usually a good starting point.\n\n### Benefits of pipelining\n\nIt provides two main benefits in terms of performance:\n- batching several commands in a single message allows us to save multiple times the round trip time between the client and the Redis server\n- it avoids context switching, both in the client and in the server. When the server or the client need to read from/write to the network, a syscall is made and an expensive context switch happens between user space and kernel space. If we send 10 messages, each with a single command, 10 context switches will happen. If we send a single message with 10 commands, it's likely that a single context switch will be needed.\n\n### Atomicity of pipelining\n\nIt is worth knowing that the part of Redis that executes client commands is single-threaded. Also, that all commands in Redis are atomic, executed individually. This means that Redis doesn't stop any command half-way through its execution to execute another command. Every individual command that is started is finished without interleaving with other commands.\n\nBut **pipelining is not atomic**. Imagine that you have 2 clients talking to the same Redis server, and both send at the same time a pipeline consisting of 5 commands each. While is guaranteed that all commands from Client 1 pipeline will be executed in order, there is no guarantee that they won't be interleaved with commands from Client 2 pipeline.\n\nThis image shows one of the multiple possible interleavings between two concurrent pipelines being executed:\n\n![pipelines interleaving](https://user-images.githubusercontent.com/4842605/69572324-ad031380-0fa2-11ea-9db2-12a5c6ac9e41.png)\n\n\nThis behavior shows that pipelining is non-blocking on the server. This means that even if a Client 1 has a huge and slow pipeline, other clients won't be blocked, because the commands from the other clients will be interleaved with the commands from Client 1 pipeline.\n\n### Limitations of pipelining\n\nThe clients only gets the command's response after executing all the pipeline. So if a client sends a pipeline with 5 commands, it will receive at the end an array with the response for each command.\n\nIf some command fails, the pipeline continues and an error is returned at the end, as the response for that specific command (not for the whole pipeline). You can test it with `printf \"SET name rafael\\r\\nINCR name\\r\\nGET name\\r\\n\" | nc localhost 6379` (note the error on the second line, but the last command gets executed anyway):\n\n```\n+OK\n-ERR value is not an integer or out of range\n$6\nrafael\n```\n\nSo you cannot, inside a pipeline, read some value from Redis and use the value later in a command inside the same pipeline.\n\n### When you should use pipelining\n\nUse pipeline when (all of the bellow):\n- you need performance\n- you have several commands to send to the Redis server\n- you don't need the response of a previous command as input for a subsequent command (because you only get all responses in the end).\n\n---\n\n## Transactions\n\nRedis transactions have a different semantics than \"transactions\" in a RDBMS context.\n\nTransactions in Redis are a mechanism for queuing commands, and later deciding whether we want to executed all of them (atomically) or give up and don't execute any command.\n\nUnlike pipelines, pipelines use special commands to mark the beginning and the end of the transaction, and the server also can queue the commands from a transaction (so the client can send one at a time). So transactions are stateful on the server, it actually keeps track of an ongoing transaction. Besides that, some libraries prefer to buffer the commands client-side and send the whole transaction inside a pipeline for optimization.\n\nA transaction works by issuing a `MULTI` command, then sending all the commands that compose the pipeline, and an `EXEC` or a `DISCARD` at the end.\n\n### Benefits of transactions\n\nIf pipelining is used on the library, all the performance benefits from using pipeline will apply.\n\nTransactions also provide a \"check-and-set\" with the `WATCH` command. Imagine the scenario:\n- before the transaction, we read a value from key `my-key` and store it in our program\n- we start the transaction with `MULTI`\n- we queue commands inside the transaction, using the value read from key `my-key`\n- we `EXEC` the transaction\n\nIn this scenario, we can use `WATCH` to avoid running the transaction with an old value of `my-key`, in the case that the value changed between we read it and we executed the transaction.\n\n### Atomicity of transactions\n\n**Transactions are atomic**. There is no interleaving of commands inside a transaction with commands from outside that transaction. The attention point is that transactions are blocking. If you have a client with a huge and slow transaction being executed, all other clients will have to wait to be served.\n\n### Limitations of transactions\n\nLike pipelining, using transactions we don't have the ability to use intermediate values for subsequent commands. We only get the whole list of responses at the end.\n\nIf an error happens in one of the commands, there are 2 possibilities:\n- if it is a syntax error (like wrong number of arguments), it is detected while queuing the commands and the transaction won't even be executed.\n- if it is a semantic error (like an operation on the wrong data type), it is only detected while executing the transaction, and (just like with pipelines), the error will be returned inside the list of responses, as the response for the specific command. But subsequent commands in the queue will be executed normally, and the transaction won't be aborted. This means that Redis doesn't have a rollback mechanism like traditional RDBMS.\n\n### When you should use transactions\n\nYou should use transactions if (all of the bellow):\n- you need atomic execution of commands\n- you don't need intermediate values to compose subsequent commands\n\n---\n\n## Lua Scripts\n\nRedis can execute client-provided scripts written in Lua. This is by far the method that opens more possibilities of the 3 presented here.\n\nA Lua script is loaded on the Redis server and can be later invoked with parameters (using the `EVALSHA` command). You can also send the whole script on every invocation (with the `EVAL` command), but you should avoid doing this for performance reasons.\n\n### Benefits of Lua scripts\n\nUnlike with pipelining and transactions, in a Lua script we can manipulate intermediate results. It is, we can read a value from Redis using a command, store the result in a Lua variable, and later use the value in a command or even in some logic like an `if` statement. We can also execute inside a Lua script any Redis command that the server supports. In the following example we use parameters (both `KEYS` and `ARGV`), issue Redis commands, read intermediate values, and use values we've read in the script logic.\n\n```lua\nlocal key = KEYS[1]\nlocal new = ARGV[1]\n\nlocal current = redis.call('GET', key)\nif (current == false) or (tonumber(new) \u003c tonumber(current)) then\n  redis.call('SET', key, new)\n  return 1\nelse\n  return 0\nend\n```\n\n### Atomicity of Lua scripts\n\n**Lua scripts are atomic**. Likewise transactions, they are blocking and can make other clients wait for a long time if the script is slow.\n\n### Limitations of Lua scripts\n\nWhile there is no limitations in terms of functionality of Lua scripts, it might be weird to use it to implement a huge pipeline with atomicity. In a pipeline we would add commands to the pipeline, going as far as thousands of commands. How to implement this with scripts? Either generate a script dynamically with the current commands, which would not perform well (because we could not reuse the script with `EVALSHA`), or make a loop inside the script and call it with a huge number (thousands) of parameters. I've never needed to do this, but just seems weird and I'm not sure it would work.\n\n### When you should use Lua scripts\n\nYou should use Lua scripts if (all of the bellow):\n- you need atomic execution of commands\n- you need intermediate values to compose subsequent commands\n- you need intermediate values to conditionally execute commands\n\n---\n\n## References\n\n- https://redis.io/topics/pipelining\n- https://redis.io/topics/transactions\n- https://redis.io/commands/eval\n- https://www.slideshare.net/RedisLabs/atomicity-in-redis-thomas-hunter\n- https://stackoverflow.com/questions/29327544/pipelining-vs-transaction-in-redis\n- https://stackexchange.github.io/StackExchange.Redis/PipelinesMultiplexers\n- https://stackexchange.github.io/StackExchange.Redis/Transactions\n","data":{"title":"Redis: Pipelining, Transactions and Lua Scripts","date":"2019-11-25T00:00:00.000Z","keywords":"redis, pipelining, transaction, lua, scripts","excerpt":"Redis offers 3 ways of grouping commands: pipelining, transactions and Lua scripts. The subtleties of using one instead of the other are explored on this post.\n"},"isEmpty":false,"excerpt":""},"slug":"redis-pipelining-transactions-and-lua-scripts"},{"document":{"content":"\nReplication and partitioning are techniques that build the foundation of using Redis as a distributed system. In this post they will be examined as very basic building blocks. For more complex needs, there are more complex abstractions, like Redis Sentinel and Redis Cluster, that build upon these building blocks.\n\n## Replication\n\nReplication means keeping multiple copies of the same data, so in case we lose one of the copies, we still can recover the data from the other copies. Here we will analyze Redis replication at its most basic level.\n\nRedis provides a basic *leader follower* replication, allowing us to have a master-replica setup. We can have one or multiple replicas for some master. This is done on the configuration file of the replica instances, using the configuration:\n\n```\nreplicaof \"my-master-hostname\" \"6379\"\n```\n\nIf you are using authentication on your Redis setup, you'll need some extra configuration to specify the password.\n\nOnce the configuration is done, each replica will stay connected to the master and receive from the master a stream of commands to create its own local copy of the dataset.\n\n### Benefits of replication\n\nThis basic replication setup has the immediate benefit of improving data safety. If you happen to lose the data on your master, you still can recover the data from your replicas. (only what was already sent to the replicas before the failure) If a replica fails or loses connection to the master, it will automatically resynchronize its data with the master once the connection is recovered. So it can help with high-availability of read-only nodes.\n\nThe second main benefit is being able to use the replicas as read-only Redis servers, and use them to make any slow O(N) queries you might need, offloading some work from the master, whose job is receiving all the writes or receiving the faster queries.\n\n### Limitations of replication\n\nReplicas are read-only by default, so write commands issued against a replica will fail. There are configurations around this, check the documentation for more information.\n\nReplication is asynchronous. This means that the master will acknowledge a write to the client before the write was successfully propagated to a replica. This creates the possibility of data loss, if the master fails after acknowledging the write but before propagating to the replica. Again, there are configurations around this to make it more safe (at the cost of worse performance).\n\nAlso, this setup does not provide any automatic failover. It is still your job to restart the master after a failure.\n\n### When you should use replication\n\nYou should use this very simple setup when (all of the bellow):\n- when you cannot afford to lose the data on your master\n- when you don't need automatic failover (if you need it, use Redis Sentinel)\n\nYou can also use it when you can benefit from having read-only replicas to offload some work from the master.\n\n## Partitioning\n\n[The docs](https://redis.io/topics/partitioning) have a really good definition of partitioning:\n\n\u003e Partitioning is the process of splitting your data into multiple Redis instances, so that every instance will only contain a subset of your keys.\n\nWhen using partitioning, some partitioning criteria is needed, to decide on which Redis server a given key should be located. The three main criterias are range partitioning, hash partitioning and consistent hashing.\n\nRedis Cluster is the state-of-the-art way of working with partitioning in Redis, but for simpler cases you might need to handle the partitioning manually.\n\nThe actual partitioning (the mapping of a key to a Redis server) can happen in different parts of the stack:\n- client side partitioning: the client computes the server from the key.\n- proxy assisted partitioning: a proxy (like [Twemproxy](https://github.com/twitter/twemproxy)) sits in between client and servers and computes the correct server from the key.\n- query routing: is the technique used by Redis Cluster, in which any Redis node can receive a query and will redirect the client to the correct node, given the partitioning scheme.\n\n### Benefits of replication\n\nPartitioning allows you to have larger datasets. If your dataset is 10Gb, but you only have nodes with 4Gb of memory, you can use 3 nodes and partition your data between the 3 nodes.\n\nAlso, partitioning allows you to split the work load between multiple nodes, scaling the computational power of your Redis setup to multiple CPUs and network adapters, instead of the single CPU and network adapter of a single-node setup.\n\n### Limitations of partitioning\n\nWhen you have keys distributed among multiple nodes, some operations involving multiple keys become useless. Like trying to compute the intersection between two sets will only work if the two keys are in the same node, which is something that the programmer should rely on. The same goes for transactions involving multiple keys.\n\nOther limitation is regarding the partitioning granularity. Given the partition granularity is the key, a single key containing a huge list or set cannot be partitioned between multiple nodes.\n\nAt last, changing the system capacity (adding or removing nodes) is hard, because there is no automatic rebalancing mechanism. Redis Cluster solves this.\n\n### When you should use partitioning\n\nYou should use this very simple partitioning when (one of the bellow):\n- when your Redis dataset is too big to fit in the memory of a single node\n- when you have a volume of requests that a single Redis node cannot keep up, and you want to distribute the load to multiple instances\n\n## References\n\n- https://redis.io/topics/replication\n- https://redis.io/topics/partitioning\n","data":{"title":"Redis: Replication and Partitioning","date":"2019-11-25T00:00:00.000Z","keywords":"redis, replication, partitioning, distributed system","excerpt":"Replication and partitioning are techniques that build the foundation of using Redis as a distributed system, and prepare the way for more complex abstractions like Redis Sentinel and Redis Cluster\n"},"isEmpty":false,"excerpt":""},"slug":"redis-replication-and-partitioning"},{"document":{"content":"\n\u003cp\u003e\u003csmall\u003e\nThis is the third and last part of a series of posts on Nexus 3 and how to use it as repository for several technologies.\n\u003c/small\u003e\u003c/p\u003e\n\n## Installation\n\nCheck out the [first part](https://rafaeleyng.github.io/using-nexus-3-as-your-repository-part-1-maven-artifacts/) of this series to see how we installed and ran Nexus 3 using a single docker command. Just do that and the installation is done.\n\n## Configuring Nexus as a Docker repo\n\nWhat we will do:\n  - create a private (hosted) repository for our own images\n  - create a proxy repository pointing to Docker Hub\n  - create a group repository to provide all the above repos under a single URL\n\nI suggest you to create a new blob store for each new repo you want to create. That way, the data for every repo will be in a different folder in `/nexus-data` (inside the Docker container). But this is not mandatory for it to work.\n\nBy default, the Docker client communicates with the repo using HTTPS. In my use case I had to configure it with HTTP, because we didn’t have the certificate nor the knowledge on how to obtain it.\n\nImportant to notice: the Docker repo requires 2 different ports. We are going to use 8082 for pull from the proxy repo and 8083 for pull and push to the private repo.\n\nI had some problems with slightly older versions of Docker, so I strongly suggesting you to start with the version that I’ve tested with, that is `1.12.3`.\n\n### private repo\n\nA repository for Docker images that your team creates.\n\nCreate a new Docker (hosted) repository and configure it like:\n\n![docker-private](https://cloud.githubusercontent.com/assets/4842605/21745036/25e11fc2-d509-11e6-85f6-6b6e016c174e.png)\n\n### proxy repo\n\nA repository that proxies everything you download from the official registry, Docker Hub. Next time you download the same dependency, it will be cached in your Nexus.\n\nCreate a new Docker (proxy) repository and configure it like:\n\n![docker-hub0](https://cloud.githubusercontent.com/assets/4842605/21745035/25daf822-d509-11e6-892b-11f79943f96b.png)\n\n![docker-hub1](https://cloud.githubusercontent.com/assets/4842605/21745034/25d9eebe-d509-11e6-9e28-fd7f63b4ae89.png)\n\n### group repo\n\nThis will group all the above repos and provide you a single URL to configure your clients to download from/deploy to.\n\nCreate a new Docker (group) repository and configure it like:\n\n![docker-group0](https://cloud.githubusercontent.com/assets/4842605/21745032/25a805de-d509-11e6-80c4-989500a2d579.png)\n\n![docker-group1](https://cloud.githubusercontent.com/assets/4842605/21745033/25c7919c-d509-11e6-89d2-88d631f343ee.png)\n\nYou can create as many repos as you need and group them all in the group repo.\n\nThis step is actually optional to use Nexus 3 as a Docker repository, because we can stick to pulling and pushing to the proxy and hosted repositories as will be discussed later.\n\n## Configuring your clients and projects to use your Nexus repos\n\nTo interact with your repo, the first thing is to configure the Docker daemon in your machine to accept working with HTTP instead of HTTPS.\n\nHow exactly to do this config depends on your operating system, so you should check [dockerd](https://docs.docker.com/engine/reference/commandline/dockerd/) documentation. On RHEL I did it putting this content in `/etc/docker/daemon.json`:\n\n```\n{\n  \"insecure-registries\": [\n    \"your-repo:8082\",\n    \"your-repo:8083\"\n  ],\n  \"disable-legacy-registry\": true\n}\n```\n\nYou have to restart the daemon after setting this (`sudo systemctl restart docker`).\n\nOn Windows or Mac you should config your deamon in a box like this:\n\n![daemon](https://cloud.githubusercontent.com/assets/4842605/21745349/f8af75b4-d510-11e6-8383-c3594b525ea4.png)\n\nNow we have to authenticate your machine to the repo with:\n\n```\ndocker login -u admin -p admin123 your-repo:8082\ndocker login -u admin -p admin123 your-repo:8083\n```\n\nRemember the default Nexus credentials, `admin/admin123`. This will create an entry in `~/.docker/config.json`:\n\n```\n{\n\t\"auths\": {\n\t\t\"your-repo:8082\": {\n\t\t\t\"auth\": \"YWRtaW46YWRtaW4xMjM=\"\n\t\t},\n\t\t\"your-repo:8083\": {\n\t\t\t\"auth\": \"YWRtaW46YWRtaW4xMjM=\"\n\t\t}\n}\n```\n\nTo pull images from your repo, use (notice port 8082 being used):\n\n```\ndocker pull your-repo:8082/httpd:2.4-alpine\n```\n\nTo push your own images to your repo, you have to tag the image with a tag that points to the repo. This is strange to me, since I was trying to think about Docker tags the same way I do about Git tags, but they seem be somewhat different (notice port 8083 being used):\n\n```\ndocker tag your-own-image:1 your-repo:8083/your-own-image:1\ndocker push your-repo:8083/your-own-image:1\n```\n\nTo pull your own images from the repo, you can use:\n\n```\ndocker tag your-own-image:1 your-repo:8082/your-own-image:1\n# or\ndocker tag your-own-image:1 your-repo:8083/your-own-image:1\n```\n\nBoth ports will work. I suspect that is because using port 8083 will connect directly to the hosted repo, whilst using port 8082 will connect to the group repo, which contains the hosted repo. I suggest you to stick to port 8083 to avoid duplicate images in your machines. If you chose to stick with port 8083 to pull your own images, you probably could skip creating the group repo, if you prefer.\n","data":{"title":"Using Nexus 3 as Your Repository - Part 3: Docker Images","date":"2017-01-07T19:37:14.000Z","keywords":"nexus, repository, private, proxy, docker","excerpt":"In this third post of the series, we will setup Nexus 3 to use it as Docker private registry and as a proxy to Docker Hub.\n"},"isEmpty":false,"excerpt":""},"slug":"using-nexus-3-as-your-repository-part-3-docker-images"},{"document":{"content":"\n\u003csmall\u003e\nThis is the second part of a series of posts on Nexus 3 and how to use it as repository for several technologies.\n\u003c/small\u003e\n\n`npm install` can take too long sometimes, so it might be a good idea to have a proxy in your own network. And if you can't just pay the 7 dollars/month to host your packages in the official npm private registry, then you'll probably benefit from this post.\n\n## Installation\n\nCheck out the [first part](https://rafaeleyng.github.io/using-nexus-3-as-your-repository-part-1-maven-artifacts/) of this series to see how we installed and ran Nexus 3 using a single docker command. Just do that and the installation is done.\n\n## Configuring Nexus as a npm repo\n\nWhat we will do:\n  - create a private (hosted) repository for our own packages\n  - create a proxy repository pointing to the official registry\n  - create a group repository to provide all the above repos under a single URL\n\nI suggest you to create a new blob store for each new repo you want to create. That way, the data for every repo will be in a different folder in `/nexus-data` (inside the Docker container). But this is not mandatory for it to work.\n\n### private repo\n\nA repository for npm packages that your team develops.\n\nCreate a new npm (hosted) repository and configure it like:\n\n![npm-private0](https://cloud.githubusercontent.com/assets/4842605/20909966/d6f8101e-bb45-11e6-9791-0f2472866fdd.png)\n\nThe deployment policy \"Allow redeploy\" above might look somewhat polemic, so you might want to set it to \"Disable redeploy\". In my use case, it makes sense to use \"Allow redeploy\", since we keep a `latest` version on Nexus always updated with the status of the master branch, that is redeployed in our CI flow.\n\n### proxy repo\n\nA repository that proxies everything you download from the official npm registry. Next time you download the same dependency, it will be cached in your Nexus.\n\nCreate a new npm (proxy) repository and configure it like:\n\n![npm-registry0](https://cloud.githubusercontent.com/assets/4842605/20909964/d6f6568e-bb45-11e6-9161-6e302ed1757f.png)\n\n![npm-registry1](https://cloud.githubusercontent.com/assets/4842605/20909965/d6f7e9ea-bb45-11e6-86f1-c6ce957bf948.png)\n\n### group repo\n\nThis will group all the above repos and provide you a single URL to configure your clients to download from/deploy to.\n\nCreate a new npm (group) repository and configure it like:\n\n![npm-group0](https://cloud.githubusercontent.com/assets/4842605/20909963/d6f41e32-bb45-11e6-9134-848409b5d781.png)\n\nYou can create as many repos as you need and group them all in the group repo, but for npm I don't think that you will need more than 1 proxy and 1 private repos.\n\n## Configuring your clients and projects to use your Nexus repos\n\nFor npm, we will configure the repository per project (unlike Maven, that have some global configs, for instance). I believe that you can configure the authentication globally in your machine, with `npm addUser`, but I didn't went that way for simplicity.\n\nIf you have a project where you only want to **download** dependencies from Nexus, create a `.npmrc` file at your project's root with:\n\n```\nregistry=http://your-host:8081/repository/npm-group/\n_auth=YWRtaW46YWRtaW4xMjM=\n```\n\n`_auth=YWRtaW46YWRtaW4xMjM=` is the base64 hash for the credentials (admin/admin123). If you use a different set of credentials, you should compute your own hash with:\n\n```\necho -n 'myuser:mypassword' | openssl base64\n```\n\nYou have to set a user so you can publish packages. If you do this from your local machine, `npm publish` will use your user configured in `~/.npmrc` (in your home, not in your project). If you don't have this configuration, or if you want to publish from CI, you can set an `email=any@email.com` configuration in your project's `.npmrc`. Really, any email.\n\nIf you have a project that you want to **publish** to your Nexus, put this in `package.json`:\n\n```\n{\n  ...\n\n  \"publishConfig\": {\n    \"registry\": \"http://your-host:8081/repository/npm-private/\"\n  }\n}\n```\n\nNote that you publish to your private repo, but when you download, you can point to your group repo, so both your own packages and the packages from the official repo will be available from a single URL.\n\nNow if you run in your projects:\n\n```\nnpm install\n# or\nnpm publish\n```\n\nyour `npm` will point to your Nexus instance.\n\n## Installing npm packages globally\n\nRun:\n\n```\nnpm --registry http://your-host:8081/repository/npm-group/ install -g your-package\n```\n","data":{"title":"Using Nexus 3 as Your Repository - Part 2: Npm Packages","date":"2016-12-03T21:31:27.000Z","keywords":"nexus, repository, private, proxy, npm","excerpt":"In this second post of the series, I'll show you how to setup Nexus 3 and configure it to use it as a private npm registry and as a proxy to the official registry.\n"},"isEmpty":false,"excerpt":""},"slug":"using-nexus-3-as-your-repository-part-2-npm-packages"},{"document":{"content":"\n\u003csmall\u003e\nThis is the first part of a series of posts on Nexus 3 and how to use it as repository for several technologies.\n\u003c/small\u003e\n\n## Installation\n\nInstall it with docker:\n\n```\ndocker run -d -p 8081:8081 -p 8082:8082 -p 8083:8083 --name my-nexus sonatype/nexus3:3.0.0\n```\n\n\u003csmall\u003e\nWe are mapping all of those ports (8081-8083) because of the next posts in the series. For this post, we'll actually only need port 8081.\n\u003c/small\u003e\n\nNexus 3 will go up on port 8081. Default credentials are admin/admin123.\n\nYou might want to create a volume to map the Nexus data folder to your host, adding the option `-v /opt/my-nexus-data:/nexus-data`.\n\n## Configuring Nexus as a Maven repo\n\nWhat we will do:\n  - create a private (hosted) repository for our snapshots\n  - create a private (hosted) repository for our releases\n  - create a proxy repository pointing to Maven Central\n  - create a group repository to provide all of these repos under a single URL\n\nI suggest you to create a new blob store for each new repo you want to create. That way, the data for every repo will be in a different folder in `/nexus-data` (inside the Docker container). But this is not mandatory for it to work.\n\n### snapshots repo\n\nA repository for Maven artifacts that you deploy **with** `-SNAPSHOT` in the end of the version tag of your pom.xml:\n\n```\n\u003cversion\u003e1.0.0-SNAPSHOT\u003c/version\u003e\n```\n\nCreate a new maven (hosted) repository and configure it like:\n\n![maven-snapshots0](https://cloud.githubusercontent.com/assets/4842605/20580349/f43cdad8-b1b8-11e6-8ff8-a9a02082197a.png)\n\n### releases repo\n\nA repository for Maven artifact that you deploy **without** `-SNAPSHOT` in the end of the version tag of your pom.xml:\n\n```\n\u003cversion\u003e1.0.0\u003c/version\u003e\n```\n\nCreate a new maven (hosted) repository and configure it like:\n\n![maven-releases0](https://cloud.githubusercontent.com/assets/4842605/20580348/f42e9964-b1b8-11e6-8e32-4a0dc717d7bf.png)\n\n### proxy to Maven Central repo\n\nA repository that proxies everything you download from Maven Central. Next time you download the same dependency, it will be cached in your Nexus.\n\nCreate a new maven (proxy) repository and configure it like:\n\n![maven-central0](https://cloud.githubusercontent.com/assets/4842605/20580346/f40f4488-b1b8-11e6-8fce-33034ef14978.png)\n\n![maven-central1](https://cloud.githubusercontent.com/assets/4842605/20580345/f40e387c-b1b8-11e6-8e4a-c314273bf1a0.png)\n\n### group repo\n\nThis will group all the above repos and provide you a single URL to configure your clients to download from/deploy to.\n\nCreate a new maven (group) repository and configure it like:\n\n![maven-group0](https://cloud.githubusercontent.com/assets/4842605/20580347/f427ce5e-b1b8-11e6-8a93-52cda1f49f59.png)\n\nYou can create as many repos as you need (like proxies to other public repos) and group them all in the group repo.\n\n\n## Configuring your clients and projects to use your Nexus repos\n\nPut this in your `~/.m2/settings.xml` file. This will configure the credentials to publish to your hosted repos, and will tell your `mvn` to use your repo as a mirror of central:\n\n```\n\u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e\n\u003csettings xmlns=\"http://maven.apache.org/SETTINGS/1.1.0\"\n  xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n  xsi:schemaLocation=\"http://maven.apache.org/SETTINGS/1.1.0 http://maven.apache.org/xsd/settings-1.1.0.xsd\"\u003e\n\n  \u003cservers\u003e\n    \u003cserver\u003e\n      \u003cid\u003enexus-snapshots\u003c/id\u003e\n      \u003cusername\u003eadmin\u003c/username\u003e\n      \u003cpassword\u003eadmin123\u003c/password\u003e\n    \u003c/server\u003e\n    \u003cserver\u003e\n      \u003cid\u003enexus-releases\u003c/id\u003e\n      \u003cusername\u003eadmin\u003c/username\u003e\n      \u003cpassword\u003eadmin123\u003c/password\u003e\n    \u003c/server\u003e\n  \u003c/servers\u003e\n\n  \u003cmirrors\u003e\n    \u003cmirror\u003e\n      \u003cid\u003ecentral\u003c/id\u003e\n      \u003cname\u003ecentral\u003c/name\u003e\n      \u003curl\u003ehttp://your-host:8081/repository/maven-group/\u003c/url\u003e\n      \u003cmirrorOf\u003e*\u003c/mirrorOf\u003e\n    \u003c/mirror\u003e\n  \u003c/mirrors\u003e\n\n\u003c/settings\u003e\n```\n\nAnd now configure your projects.\n\nIf you want only to download dependencies from Nexus, put this in the `pom.xml`:\n\n```\n\u003cproject ...\u003e\n\n  ...\n\n  \u003crepositories\u003e\n    \u003crepository\u003e\n      \u003cid\u003emaven-group\u003c/id\u003e\n      \u003curl\u003ehttp://your-host:8081/repository/maven-group/\u003c/url\u003e\n    \u003c/repository\u003e\n  \u003c/repositories\u003e\n\u003c/project\u003e\n```\n\nAnd if you want also to publish your project, add:\n\n```\n\u003cproject ...\u003e\n\n  ...\n\n  \u003cdistributionManagement\u003e\n    \u003csnapshotRepository\u003e\n      \u003cid\u003enexus-snapshots\u003c/id\u003e\n      \u003curl\u003ehttp://your-host:8081/repository/maven-snapshots/\u003c/url\u003e\n    \u003c/snapshotRepository\u003e\n    \u003crepository\u003e\n      \u003cid\u003enexus-releases\u003c/id\u003e\n      \u003curl\u003ehttp://your-host:8081/repository/maven-releases/\u003c/url\u003e\n    \u003c/repository\u003e\n  \u003c/distributionManagement\u003e\n\u003c/project\u003e\n```\n\nNow if you run in your projects:\n\n```\nmvn install\n# or\nmvn deploy\n```\n\nyour `mvn` will point to your Nexus instance.\n","data":{"title":"Using Nexus 3 as Your Repository - Part 1: Maven Artifacts","date":"2016-11-23T21:55:08.000Z","keywords":"nexus, repository, private, proxy, maven","excerpt":"Learn how to setup Nexus 3 and configure it to use it both as a Maven private repository and as a proxy to Maven Central and other repos\n"},"isEmpty":false,"excerpt":""},"slug":"using-nexus-3-as-your-repository-part-1-maven-artifacts"},{"document":{"content":"\nCabot is a application monitoring service. Basically a system that you deploy to your own infrastructure and configure it to check the URLs of your applications at a given interval, and possibly enable some kind of alert when things go down.\n\nIt looks like this:\n\n![screen shot 2016-11-19 at 11 23 48 pm](https://cloud.githubusercontent.com/assets/4842605/20459783/9930e04a-aeb5-11e6-8fed-1ab2aa866d95.png)\n\nThere's a lot of services that offer similar functionality ([Pingdom](https://www.pingdom.com/), [Uptime Robot](https://uptimerobot.com/)), but Cabot is self-hosted, open-source and free, so you don't have to worry about constraints like a minimum check interval or a maximum number of applications that you can monitor.\n\nThis is a pretty basic \"get started\" that shows how to quickly get Cabot working. I will show only one way of doing each thing.\n\n## Installation\n\nInstall with docker, using [cabot-docker](https://github.com/shoonoise/cabot-docker):\n\n1. Install docker + docker-compose.\n\n1. `git clone https://github.com/shoonoise/cabot-docker.git`\n\n1. `cd cabot-docker` and take a look at `cabot_env` file. There's a lot of variables you can define there to configure your alerts and other integrations, just comment all of them so we don't have invalid values later.\n\n1. `docker-compose up -d`\n\nIt should go up on port 8080 (change it on docker-compose.yml if you want). The default credentials are docker/docker.\n\n## Concepts\n\n3 major concepts in Cabot: check, instance and service.\n\n**Check** is some particular task you want to run to check something. Checks can be of some predefined types, like:\n  - ping: a ping to a host\n  - HTTP: call an URL and check the HTTP status.\n\n**Instance** is an actual instance of a machine that will have some service running. It will have a IP/hostname.\n\n**Service** is the macro stuff you want to monitor.\n\nCabot's documentation is pretty obscure about those concepts, so here is my take on how to use them.\n\n## Configuring\n\nThere's an N-N relationship between those 3 entities, so it can be pretty confusing how to organize and relate everything. This post is pretty opinionated about how to do it.\n\nYou can have a **service** with multiple **instances**, and multiple **checks** pointing to each instance.\n\nI'm using Cabot to monitor both my front-end servers and my API, of multiple applications, running each on multiple servers.\n\nI start by creating my **instances**. Every newly created instance will have an IP/hostname, and will automatically create a ping **check** to it. For each instance, I create an HTTP **check** that checks for HTTP status and some text match on the return. Then I create a **service** and group on it all of its instances.\n\n## Automating\n\nClicking is tedious, error prone and slow. Mostly, it is the wrong way.\n\nUnfortunately, Cabot doesn't have a REST API that we can use to configure our services automatically.\n\nThe way I see it, we have 2 options:\n\n- [Edit 2016-12-23: deprecated. See cabot-db-config bellow] I've created [cabot-zombie](https://github.com/rafaeleyng/cabot-zombie), that spins up a headless browser and configures Cabot based on a Javascript object. Check out the project for more details.\n\n- Access the Postgres instance that Cabot uses and make some inserts directly. The tables involved are:\n  - cabotapp_instance_status_checks\n  - cabotapp_instance\n  - cabotapp_instancestatussnapshot\n  - cabotapp_service_instances\n  - cabotapp_service_status_checks\n  - cabotapp_service\n  - cabotapp_servicestatussnapshot\n  - cabotapp_statuscheck\n  - cabotapp_statuscheckresult\n\n- I've created [cabot-db-config](https://www.npmjs.com/package/cabot-db-config), which uses a configuration similar to cabot-zombie, but is way faster and more reliable, because it connects directly to your Cabot database to insert the configuration.\n\nActually what I use currently is cabot-db-config to create stuff, and if I want to change something, I just delete everything from the tables above and rerun cabot-zombie with my changes.\n","data":{"title":"Monitoring Your Application Status With Cabot","date":"2016-11-19T21:05:26.000Z","keywords":"cabot, monitoring, status, ping","excerpt":"How to setup a status check for your applications\n"},"isEmpty":false,"excerpt":""},"slug":"monitoring-your-application-status-with-cabot"},{"document":{"content":"\n## Code coverage\n\nIf you want to set up code coverage measurements in a project, there are usually 4 things that you'll need:\n\n- a **test runner** (like [Mocha](https://github.com/mochajs/mocha), [AVA](https://github.com/sindresorhus/ava)), to run your tests\n- a **code coverage** tool (like [Istanbul](https://github.com/gotwarlost/istanbul), [nyc](https://github.com/bcoe/nyc)), to generate code coverage reports\n- a **code coverage insight** service (like [Coveralls](https://coveralls.io/), [Codecov](https://codecov.io/)), to provide you a nice visualization of your code coverage data\n- a **reporting** tool (like [node-coveralls](https://github.com/nickmerwin/node-coveralls), [codecov.io](https://github.com/cainus/codecov.io/)), to send your reports to some service\n\n## Assumptions\n\nI'll assume you already have Mocha locally installed (`npm i -D mocha`) as your test runner, and show you how to do the rest.\n\nI'll also assume that you have TravisCI server configured for your project.\n\n### npm scripts\n\nnpm scripts are scripts you add inside package.json. You can add arbitrary shell script commands there, like: `\"scripts\": { \"print-something\": \"echo 'something'\" }`. You can run this script by invoking it by its name: `npm run print-something`.\n\nI try to use npm scripts as much as possible, to avoid global dependencies and to avoid the clutter to manually invoke local dependencies. So all examples here will use this approach.\n\n\n## Code coverage tool\n\nThe principle is: instead of directly running your tests, you set up a middle man to do this and generate the coverage reports. Enter Istanbul.\n\n```sh\nnpm i -D istanbul\n```\n\nIn npm scripts:\n\n```json\n\"coverage\": \"./node_modules/istanbul/lib/cli cover _mocha test/index.js\"\n```\n\nor, if you are using ES6 in your tests, do `npm i -D babel-cli` and run it using babel-node:\n\n```json\n\"coverage\": \"babel-node ./node_modules/istanbul/lib/cli cover _mocha test/index.js\"\n```\n\nNote that `_mocha`, with underscore, is the actual runner. `mocha` seems to be just a wrapper around it.\n\nRun this script with `npm run coverage` and it will generate a `coverage` folder with illegible coverage data, and print some results to the console. You should put this folder in .gitignore, with `echo coverage \u003e\u003e .gitignore`.\n\n\n## Code coverage insight service\n\nJust sign up to [Coveralls](https://coveralls.io/) and add the Github repo you want to watch.\n\n\n## Coverage reporting tool\n\nInstall node-coveralls with:\n\n```sh\nnpm i -D node-coveralls\n```\n\nThis will install a tool that knows how to get you `coverage` folder and report it to the Coveralls service.\n\nBut you are not going to make those reports manually. Instead, your CI server will do this. Since I'm assuming you are using TravisCI, you should already have a `.travis.yml` file. Add:\n\n```yml\nafter_success:\n  - npm run coverage\n  - npm run report-coverage\n```\n\n`coverage` is the script that creates the coverage report. `report-coverage` is the script that takes that data and reports it to the service. Since this is running in your CI server, set it up to run for every pull request/push.\n\nThe script for `npm run report-coverage` is:\n\n```json\n\"report-coverage\": \"cat ./coverage/lcov.info | coveralls\"\n```\n\nThe combination of Coveralls + TravisCI makes that you don't have to provide any kind of token for the two services communicate. If you choose different services, you might need an extra step of configuring some access token.\n\nExtra: if you choose to use Codecov, you should use instead:\n\n```sh\nnpm i -D codecov.io\n```\n\nand\n\n```json\n\"report-coverage\": \"cat ./coverage/lcov.info | codecov\"\n```\n\n## Extra: checking coverage as part of your CI\n\nWant to make your build fail if your coverage falls bellow some threshold? Add an extra script to your `.travis.yml` to be run in your CI build:\n\n```yml\nscript:\n  # ...\n  - npm run check-coverage\n```\n\nand define that script in `package.json`:\n\n```json\n\"check-coverage\": \"istanbul check-coverage --statements 70 --branches 70 --functions 70 --lines 70\",\n```\n\nThose `70`s are the percentage of coverage that you want as a threshold. Your build will fail if those thresholds are not met.\n","data":{"title":"JavaScript Code Coverage With Instanbul and Coveralls","date":"2016-01-14T23:50:18.000Z","keywords":"test, coverage, javascript, npm","excerpt":"A fast lane to set up code coverage in your project\n"},"isEmpty":false,"excerpt":""},"slug":"javascript-code-coverage-with-instanbul-and-coveralls"},{"document":{"content":"\n## git hooks\n\ngit provides a way to *hook* some git actions (like commit, push) to run custom scripts. These hooks run at different moments of the actions, and you can use them to improve your workflow in several ways. Some examples are preventing a commit to happen if a test fails, or running some kind of build tool before pushing.\n\nWhen you create a git project, you'll have a bunch of hook samples inside `.git/hooks`. None of these are actually run because they have the `.sample` extension. Remove that extension and the hook starts to get called.\n\n## What's wrong with git hooks\n\nYour git hooks stay inside the `.git` folder. Since this folder contains your local project versioning history, it is not versioned. The problem is that even if you create a hook file, you can't commit this file and push it to your remote, so this file won't exist for the rest of your team.\n\n## ghooks to the rescue!\n\nStart by installing ghooks:\n\n```sh\nnpm i -D ghooks\n```\n\nWhen you do that, ghooks will create a whole lot of files inside your `.git/hooks` dir. But that is not a problem anymore, because everybody that clones your project will also have those files created when they run `npm install`. And all those hook files are identical: they simply require ghooks and call it passing the hook file's own name. So supposing we are to run a commit, git calls the hook, but the hook calls ghooks, saying \"hey, git are running the pre-commit hook, handle it at your will\".\n\nghooks will then look for how you want to handle that hook. The way to go is define this is in your `package.json`:\n\n```js\n\"config\": {\n  \"ghooks\": {\n    \"pre-commit\": \"npm run test\"\n  }\n}\n```\n\nEven though your git hooks (auto generated by ghooks) are still buried inside `.git` folder, the actual code they will execute is not. Now you can keep this code versioned and consistent across your team.\n\n### A little bit of shell scripting\n\nPrograms are called, run, and exit. The program exits with a code number that tells whether it ran successfully or not. `0` is success. Any other code is an error.\n\nSome git hooks have the power to actually prevent the git action to be performed. *pre-commit* is one of them. If your hook exits non-zero, the commit won't happen. This way, you can run your linter or tests before commiting, and if they fail, the task runner will exit non-zero and the commit won't happen.\n\n### A gotcha with exit codes\n\nYou can actually run more than one command in your hook. For instance, you could run `npm run test; npm run build`.\n\nYou have to be aware that, for git, it only matters the exit code of the last program of your hook. So even if `npm run test` fails, `npm run build` could succeed and the commit will happen.\n\nI ran into this myself. I was in a situation where I had to commit a bundled JavaScript file in my project. Before commiting, I wanted my hook to run my linter, my tests and to bundle the file with production configuration. But the bundled file would need to be added to my git index, so I needed to run `git add dist/bundle.js`. I created a `pre-commit` npm script that would call my appropriate Gulp tasks and ended up with a pre-commit ghook like `npm run pre-commit; git add dist/bundle.js`.\n\nYou see the problem? Even if my linter or tests failed, or even if my bundler (Webpack) failed to bundle my file, `git add dist/bundle.js` would succeed and exit with `0`. My commit **would** happen, and it **shouldn't**.\n\nThe solution was to use a shell script `if` (if you don't know how a shell script `if` works, I warn you that is not like you expect from other programming languages):\n\n```javascript\n  \"pre-commit\": \"if npm run pre-commit; then git add dist/bundle.js; else printf 'pre-commit error: fix the test and/or lint errors and commit again'; (exit 1); fi\"\n```\n\nSee the `(exit 1)` there? That will make my hook exit non-zero and make my commit not happen.\n\n### Skipping hooks\n\nLet's say you added a line to your README. You don't need to run lint/tests/build. Just use:\n\n```sh\ngit commit -m 'message' --no-verify\n```\n\n## List of git hooks\n\nCheck the [docs](https://git-scm.com/docs/githooks).\n","data":{"title":"Simple Git Hooks With Ghooks","date":"2016-01-10T00:18:59.000Z","keywords":"git, hooks, workflow","excerpt":"Learn how to use git hooks easily to leverage best practices in your workflow.\n"},"isEmpty":false,"excerpt":""},"slug":"simple-git-hooks-with-ghooks"},{"document":{"content":"\n## Jekyll + gh-pages\n\nIf you have some static site generated with [Jekyll](https://jekyllrb.com/), chances are that you are hosting your site with Github [gh-pages](https://help.github.com/articles/using-jekyll-with-pages/).\n\ngh-pages are awesome for their simplicity to host and deploy your static site, but they lack the ability to allow you customize your build process. Github won't allow you to run arbitrary Jekyll plugins at their environment, because of security concerns.\n\nIn this post I've collected a bunch of techniques to optimize your assets at build time that you can use even when hosting your Jekyll site in gh-pages.\n\n\n## Optimize HTML\n\n### Minify HTML with Compress\n\n[Jekyll Compress HTML](https://github.com/penibelst/jekyll-compress-html) is a Jekyll layout file that compresses HTML by removing unnecessary empty spaces characters and by removing [optional HTML tags](http://www.w3.org/TR/html5/syntax.html#optional-tags).\n\nIs just a [single HTML file](https://github.com/penibelst/jekyll-compress-html/blob/master/site/_layouts/compress.html) that you download and put inside your `_layouts` folder. Then you reference it by putting\n\n```\n---\nlayout: compress\n---\n```\n\nin the front-matter of your default layout, or in your HTML files. You can also remove empty spaces from a JSON/XML file that you generate, using the same approach.\n\nYou can check how it's done in [this commit](https://github.com/codeheaven-io/codeheaven.io/commit/96187be6c5c96c4785243c9ebf194823f5db9a35).\n\n## Optimize CSS\n\nJekyll natively supports Sass, so you can concatenate your files into a single bundle out-of-the-box. But only doing this won't minify or inline your CSS.\n\n### Inline CSS\n\nIt is a good idea to inline the CSS that is used to render the above-the-fold content of your site. But if you're running a simple blog or project page, probably your above-the-fold CSS is *all* of your CSS. So it might be a good idea to inline your whole CSS into a `\u003cstyle\u003e` tag in the head of your page. Specially if your HTML + inline CSS together are smaller that 14.6kb gzipped (gh-pages serves your site gzipped), because this is [how much data your client can receive in its first roundtrip to the server](https://developers.google.com/speed/docs/insights/mobile?hl=en).\n\nTo do this, you create a `inline.scss` file your `_includes` directory, and import whatever SCSS file you want to end up being inlined. Usually, will be as simple as importing a `main.scss` file (from your `_sass` directory):\n\n```\n@import \"main\";\n```\n\nThen, we include this file inside a `\u003cstyle\u003e` tag in the head of the document, passing it through Jekyll's built-in [scssify](http://www.rubydoc.info/github/jekyll/jekyll/Jekyll/Filters:scssify) filter:\n\n{% raw %}\n```\n{% capture inline_css %}\n  {% include inline.scss %}\n{% endcapture %}\n{{ inline_css | scssify }}\n```\n{% endraw %}\n\nYou can check how it's done in [this commit](https://github.com/codeheaven-io/codeheaven.io/commit/12ed5810d2edf6a967154cd14ee77b69ccf25c7f).\n\nOf course you can choose to inline only a portion of your CSS and load a separate CSS file.\n\n\u003csmall\u003e\nThis technique I learned at this [Kevin Sweet post](http://www.kevinsweet.com/inline-scss-jekyll-github-pages/).\n\u003c/small\u003e\n\n### Minify inline CSS\n\nRemember our friend Jekyll Compress HTML? If your CSS is inline, Compress will remove empty spaces, which is basically how CSS is minified.\n\n## Optimize JS\n\n### Concatenate JS\n\nLet's save some HTTP requests. Write all your JS in as many files you want and `include` all of them in a single file, and add a single `\u003cscript\u003e` tag in your HTML requesting this file.\n\nJekyll's `include` will only look at the `_includes` folder, so it you want to keep your JS files in a separate folder, use [`include_relative`](http://jekyllrb.com/docs/templates/), which allows you to include relative to the file you are working on.\n\nYou can check how it's done in [this commit](https://github.com/CWISoftware/eventos/commit/b180160afb613287c50bcc2f8f411fc4fe0d6fe0).\n\n### Minify JS\n\nThis is what I haven't discovered yet how to do without a plugin. If you know, please tell me in the comments box bellow.\n\nJekyll Compress won't do it: it only removes empty space, and that's not how JS is minified. Besides, if you use a `//` for a comment, all of your code after that will be commented out.\n\n### Inline JS\n\nA simple `include` or `include_relative` of your file inside a `\u003cscript\u003e` tag will do it. But it **won't work** with Jekyll Compress HTML, so pick which is better for your case.\n","data":{"title":"Optimizing Assets in Jekyll-generated gh-pages","date":"2015-10-31T00:28:39.000Z","excerpt":"Learn how to optimize assets for your Jekyll site without having to rely on plugins.\n"},"isEmpty":false,"excerpt":""},"slug":"optimizing-assets-in-jekyll-generated-gh-pages"},{"document":{"content":"\n## Introduction\n\nFunctional Programming lies somewhere in the middle-ground between Computer Science and Mathematics. This is a pretty dense and theoric subject, so my goal here is to show you how to start using right away some classic functions for collections manipulation in your everyday code, focusing in practical examples of when you might use them.\n\nIn this post I'll show you how to use the 7 classical functions that I find most useful in collections manipulation:\n\n* [each](#each)\n* [filter](#filter)\n* [find](#find)\n* [some](#some)\n* [every](#every)\n* [map](#map)\n* [reduce](#reduce)\n\nUsing these simple functions you'll find yourself writting a lot less of boilerplate code. Kiss goodbye a lot of `for` loops and `if` tests. Focus your code on your business logic, not on collection traversal and array accessing.\n\n\u003csmall\u003e\n(The examples are in Javascript, but you'll find these principles applicable to many other languages like Ruby, Java 8 lambdas and C#, with LINQ. Although I will manually implement all the functions to show you the undercovers, many of these functions are already implemented in native Javascript arrays and many Javascript libs. We will manipulate some hypothetical \u003ca href=\"https://gist.githubusercontent.com/rafaeleyng/e381da0b19039531dd33/raw/1ae6dc1e28ae2169110bfa5c9077ddb322065169/ch-functional-programming-data.js\" target=\"_blank\"\u003eorders data\u003c/a\u003e in the examples.)\n\u003c/small\u003e\n\n\u003c!-- ======================================================================= --\u003e\n\n## each\n\nYou've been there. You have a collection of things, and you want to do something with `each` one of them.\n\n**Use case:** I want set each one of my orders as finished.\n\n```javascript\nfor (var i = 0; i \u003c orders.length; i++) {\n  orders[i].finished = true;\n}\n```\n\n**Basic implementation:** `each` is a function that operates on a collection. For each item of this collection, a function will be called, receiving this item as a parameter. Inside this function, you do whatever you want with that item.\n\n```javascript\nvar each = function(list, operation) {\n  for (var i = 0; i \u003c list.length; i++) {\n    operation(list[i]);\n  }\n}\n```\n\nRewriting the example using `each`:\n\n```javascript\neach(orders, function(order) {\n  order.finished = true;\n});\n```\n\nWe don't worry about collection traversal or accessing elements using indexes. We just worry about what we want to do for each item.\n\n\u003c!-- ======================================================================= --\u003e\n\n## filter\n\nYou have a collection of things, and you want a collection with only the things that have some characteristic of your interest.\n\n**Use case:** I want a list with the orders of the customer with code 1.\n\n```javascript\nvar ordersCustomer1 = [];\nfor (var i = 0; i \u003c orders.length; i++) {\n  if (orders[i].customer === 1) {\n    ordersCustomer1.push(orders[i]);\n  }\n}\n```\n\n**Basic implementation:** `filter` is a function that, for each item of a collection, calls a function passing that item as a parameter. This function should use that item to test some characteristic of this item (a *predicate*), and return a boolean that indicates whether it has or has not that characteristic. All items that have the characteristic will be accumulated in a new collection and returned by `filter`.\n\n```javascript\nvar filter = function(list, operation) {\n  var results = []\n  for (var i = 0; i \u003c list.length; i++) {\n    if (operation(list[i])) {\n      results.push(list[i]);\n    }\n  }\n  return results;\n}\n```\n\nRewriting the example using `filter`:\n\n```javascript\nvar ordersCustomer1 = filter(orders, function(order) {\n  return order.customer === 1;\n});\n```\n\n\u003c!-- ======================================================================= --\u003e\n\n## find\n\n`find` is a `filter` with 2 differences: you filter using a unique identifier, and it returns a single item (not a collection).\n\n**Use case:** I want to find the order with code 2.\n\n```javascript\nvar order2;\nfor (var i = 0; i \u003c orders.length; i++) {\n  if (orders[i].number === 2) {\n    order2 = orders[i];\n    break;\n  }\n}\n```\n\n**Basic implementation:** `find` is a function that, for each item of a collection, calls a function passing that item as a parameter. This function should use that item to test some unique identifier of this item (a *predicate*), and return a boolean that indicates whether it has or has not that identifier. `find` will stop searching as soon as it finds the first item that matches the predicate and will return this item.\n\n```javascript\nvar find = function(list, operation) {\n  for (var i = 0; i \u003c list.length; i++) {\n    if (operation(list[i])) {\n      return list[i];\n    }\n  }\n}\n```\n\nRewriting the example using `find`:\n\n```javascript\nvar order2 = find(orders, function(order) {\n  return order.number === 2;\n});\n```\n\n\u003c!-- ======================================================================= --\u003e\n\n## some\n\nYou want to know if `some` of your items has some characteristic. It can be only one, two, or it can be all of them.\n\n**Use case:** I want to know whether exists a not finished order.\n\n```javascript\nvar existsNotFinished = false;\nfor (var i = 0; i \u003c orders.length; i++) {\n  if (!orders[i].finished) {\n    existsNotFinished = true;\n    break;\n  }\n}\n```\n\n**Basic implementation:** `some` is a function that, for each item of a collection, calls a function passing that item as a parameter. This function should use that item to return a boolean value that indicates whether the item has the characteristic. `some` will return `true` as soon as it finds some item that has the characteristic, of `false` if none does.\n\n```javascript\nvar some = function(list, operation) {\n  for (var i = 0; i \u003c list.length; i++) {\n    if (operation(list[i])) {\n      return true;\n    }\n  }\n  return false;\n}\n```\n\nRewriting the example using `some`:\n\n```javascript\nvar existsNotFinished = some(orders, function(order) {\n  return !order.finished;\n});\n```\n\n\u003c!-- ======================================================================= --\u003e\n\n## every\n\nYou want to know if `every` of your items has some characteristic.\n\n**Use case:** I want to know whether all orders are not finished.\n\n```javascript\nvar allFinished = true;\nfor (var i = 0; i \u003c orders.length; i++) {\n  if (!orders[i].finished) {\n    allFinished = false;\n    break;\n  }\n}\n```\n\n**Basic implementation:** `every` is a function that, for each item of a collection, calls a function passing that item as a parameter. This function should use that item to return a boolean value that indicates whether the item has the characteristic. `every` will return `false` as soon as it finds some item that doesn't have the characteristic, of `true` if all of them do.\n\n```javascript\nvar every = function(list, operation) {\n  for (var i = 0; i \u003c list.length; i++) {\n    if (!operation(list[i])) {\n      return false;\n    }\n  }\n  return true;\n}\n```\n\nRewriting the example using `every`:\n\n```javascript\nvar allFinished = every(orders, function(order) {\n  return order.finished;\n});\n```\n\n\u003c!-- ======================================================================= --\u003e\n\n## map\n\nFor each item of your collection you want to obtain a value by transforming that item, and you want a collection of these values back.\n\n**Use case:** I need a list containing only the numbers of my orders (I want to *transform* my orders into its numbers).\n\n```javascript\nvar orderNumbers = [];\nfor (var i = 0; i \u003c orders.length; i++) {\n  orderNumbers.push(orders[i].number);\n}\n```\n\n**Basic implementation:** `map` is a function that, for each item of a collection, calls a function passing that item as a parameter. This function should use that item to obtain some value from it (getting one of its properties, making some kind of calculation etc) and return this value. `map` will accumulate those values in a collection and return this collection.\n\n```javascript\nvar map = function(list, operation) {\n  var results = [];\n  for (var i = 0; i \u003c list.length; i++) {\n    results.push(operation(list[i]));\n  }\n  return results;\n}\n```\n\nRewriting the example using `map`:\n\n```javascript\nvar orderNumbers = map(orders, function(order) {\n  return order.number;\n});\n```\n\n\u003c!-- ======================================================================= --\u003e\n\n## reduce\n\nFrom your collection, you want to get to some value that somehow represents it, like some kind of totalization.\n\n**Use case:** I want to know the total cost of my order.\n\n```javascript\nvar orderTotal = 0;\nvar order = orders[0]\nfor (var i = 0; i \u003c order.items.length; i++) {\n  var item = order.items[i];\n  orderTotal += item.value * item.quantity;\n}\n```\n\n**Basic implementation:** `reduce` is a function that, for each item of a collection, calls a function passing that item and an accumulator value as parameters. This function should use that item and the accumulator to calculate some value, that will be the new value of the accumulator. `reduce` receives an initial accumulator value, that for addition will be usually `0`.\n\n```javascript\nvar reduce = function(list, operation, initial) {\n  var accumulator = initial;\n  for (var i = 0; i \u003c list.length; i++) {\n    accumulator = operation(accumulator, list[i]);\n  }\n  return accumulator;\n}\n```\n\nRewriting the example using `reduce`:\n\n```javascript\nvar orderTotal = reduce(orders[0].items, function(acc, item) {\n  return acc + (item.value * item.quantity);\n}, 0)\n```\n\n\u003c!-- ======================================================================= --\u003e\n\n## Combining functions\n\nI want to show you an example of how these functions can be combined to make your code more concise and expressive. Suppose we want to obtain a list with the total value of each of the orders. We could do this:\n\n```javascript\nvar totals = [];\nfor (var i = 0; i \u003c orders.length; i++) {\n  var order = orders[i];\n  var total = 0;\n  for (var j = 0; j \u003c order.items.length; j++) {\n    var item = order.items[j];\n    total += item.value * item.quantity;\n  }\n  totals.push(total);\n}\n```\n\nGenerically, what we want to do is to `map` each order to a function that will `reduce` the value of its items multiplied by the quantities into a single value:\n\n```javascript\nvar totals = map(orders, function(order) {\n  return reduce(order.items, function(acc, item) {\n    return acc + (item.quantity * item.value);\n  }, 0);\n});\n```\n\n## Further reading\n\nIf you want some nice reading about Functional Programming, check out [Professor Frisby's Mostly Adequate Guide to Functional Programming](https://github.com/DrBoolean/mostly-adequate-guide).\n\nSome video courses on the subject: [Programming Languages](https://www.coursera.org/course/proglang) and [Functional Programming Principles in Scala](https://www.coursera.org/course/progfun).\n\nI also recommend you to read some real world implementations of the functions explained here in [Underscore.js](http://underscorejs.org/) source code.\n","data":{"title":"Practical Guide to Functional Programming","date":"2015-07-22T01:46:46.000Z","excerpt":"Functional Programming lies somewhere between Computer Science and Mathematics. It is a pretty dense and theoric subject, so the goal here is to show you how to start using some classic functions for collection manipulation in your everyday code.\n"},"isEmpty":false,"excerpt":""},"slug":"practical-guide-to-functional-programming"}]}},"page":"/","query":{},"buildId":"38LWUFnuUIJvM1_FqAo7H","nextExport":true}</script><script nomodule="" src="/_next/static/runtime/polyfills-a89f9ba1997a5c81dcc9.js"></script><script async="" data-next-page="/" src="/_next/static/38LWUFnuUIJvM1_FqAo7H/pages/index.js"></script><script async="" data-next-page="/_app" src="/_next/static/38LWUFnuUIJvM1_FqAo7H/pages/_app.js"></script><script src="/_next/static/runtime/webpack-9369c5c69dbf6d4912cb.js" async=""></script><script src="/_next/static/chunks/commons.e65592b2d799ba7332ae.js" async=""></script><script src="/_next/static/runtime/main-b1511fc684da8c38d25b.js" async=""></script></body></html>